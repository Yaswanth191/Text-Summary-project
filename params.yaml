#TrainingArguments:
 # num_train_epochs: 1,warmup_steps: 500,per_device_train_batch_size: 1
 # weight_decay: 0.01,logging_steps: 10,evaluation_strategy: steps, eval_steps: 500
  #save_steps: 1e6,gradient_accumulation_steps: 16


  TrainingArguments:
  num_train_epochs: 1  # For quick testing
  warmup_steps: 100   # Lower warmup due to fewer training steps
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  weight_decay: 0.01
  logging_steps: 10
  evaluation_strategy: "steps"
  eval_steps: 50       # More frequent evaluations
  save_steps: 100      # Save checkpoint every 100 steps
  save_total_limit: 2  # Keep only 2 checkpoints
  fp16: false         # Disable mixed precision since on CPU
  max_steps: 20 